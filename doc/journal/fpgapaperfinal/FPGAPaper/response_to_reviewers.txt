We would like to thank the reviewers for their detailed and constructive feedback. We believe we have addressed
all the reviewer comments (or explained below why we did not implement a suggestion) and believe this
has made a stronger paper. Significant text changes are highlighted in yellow in the paper, and 
we summarize the changes made to address each comment or answer any detailed questions in the response
below.

Reviewer: 1

The paper presents significant space talking about how to transform a hyper graph to a graph using various models.  This has been well studied in the past.  It would be useful to present the proposed techniques in the context of this previous work.  As a start, there are numerous references within reference [22] that should be considered.  Doing so would help the reader understand the novelty of the hypergraph->graph methodology and results.

[Javeed to add survey paper reference and discussion on why we use a non-dummy node star topology][DONE]

The partitioning flow does not appear to be timing driven.  Given that, I wonder how meaningful the results in Section VI-C (Circuit speed vs. interposer delay) are.  These results show that the critical path delay is strongly affected by the delay_increase, but any reasonable timing-driven CAD flow would try to keep the critical path on one side of the partition to minimize the number of critical path segments crossing a partition.  Can you comment on this?  Can you at least provide data regarding how many cut crossings are typical on the critical paths?

[Javeed add table on number of cut crossings on critical paths vs. total connections on critical paths.  Also some discussion on timing-driven partitioning -- our star model should help timing, and a timing-driven partitioner might help further but will the challenge of estimating critical paths accurately is hard.  Could reference Mike Hutton paper on APEX 20k timing-driven partitioning.

Ref is:  Hutton, M., Adibsamii, K. and Leaver, A. “Adaptive Delay Estimation for Partitioning-Driven PLD Placement”, IEEE Trans. VLSI, 11:1, pp. 60-63, 2003.

My summary of the paper is: 
"A commercial recursive partitioning placement algorithm for the Altera Apex 20K family is described in [92]. Apex has a hierarchical routing architecture, making it well suited to partitioning-based placement. Recursive partitioning is conducted along the natural cut-lines formed by the various hierarchy levels of the routing architecture, as shown in Figure 13. Notice that the sequence of partitions in this algorithm is significantly different than that of ALTOR, showing the large impact an FPGA’s routing architecture has on placement algorithms. This algorithm is made timing-driven by weighting connections with low slack highly during each partitioning phase to encourage partitioning solutions in which such connections can be routed using only fast, lower-hierarchy-level routing. To improve the prediction of the critical path, the delay estimate for each connection is a function both of the known number of hierarchy boundaries the net must traverse due to partitionings at the higher levels of the routing hierarchy, and statistical estimates of how many hierarchy boundaries the connection will cross at future partitioning steps. "] [DONE]


A key result that would help the reader interpret the results would be a table that shows, for each benchmark circuit, the number of signals that cross the partition (independent of the place and route).   It is important to break this down by benchmark circuit.  If one was to build such a device, it would be important to consider the range of cut requirements rather than just the average.  This is especially important considering the results of Figure 23 which suggests that increasing the channel width is not an efficient way to make up for a lack of cut capacity.

[Javeed to get table and add to paper if feasible.  Try to add in same table as critical path one.  Best flow, 2 partitions, cut net count vs. benchmark and cut net per channel vs. benchmark.] [DONE, but p&r dependent]

In Figure 7, it was not clear to me why, for 30% of the wires cut, the placer optimizations do worse than the placer without optimizations.  Is this just experimental noise?

[ This is due to experimental noise; the placer optimizations simply appear to be unnecessary for a small % wires cut so we get small random variations of the same magnitude as those due to changing the placement random seed.]

Did you adjust the range windows in VPR to account for the new architecture?

[ No, we did not adjust the range windows.  As the range window initially covers the entire FPGA and gradually shrinks to a box that allows moves only to adjacent blocks of the compatible type, VPR's natural range limit still permits full flexibility in moving across the interposers, even at the end of the anneal.  We agree it is a possible direction for future work to see if altering the move generation functions could save CPU time. ]

Section III-E contains a lot of information about the detailed implementation of the model within VPR.  While this is useful to someone who wants to replicate the results, if it becomes necessary to cut content to address reviewer comments, some of this detail could be omitted without losing much from the paper (the fact that an extra RR_Graph node is created for each crossing is likely not as interesting to most readers, compared to the other experimental results).

[ Ehsan: after we see how long new material makes the paper Ehsan could shorten this section.]

I wasn’t clear on how useful the results in VI-E were.   The experiments in the rest of the paper assumed a square FPGA in VPR (before cutting) so more like Figure 27(a). The alternative in Figure 27b intuitively seems like a bad idea: (a) it is known that highly rectangle FPGAs (more columns than rows) lead to higher channel width, and (b) the number of interposers is lower, which from the rest of the paper we know is a bad idea.  So the fact that Figure 27b gives worse results should not be surprising.  If there is something else to conclude from this table, perhaps it could be highlighted.

[ Ehsan: could shorten if we run out of space. ]


Reviewer: 2

1. In section IV A (Placer Optimization), you have assumed that all the FPGA dies are homogeneous. Can you explain a bit about how would you handle heterogeneous dies? For example, there can be 4 dies in a chip and 1 of them is DSP-heavy, another one is Block-RAM heavy and other two are "regular" with lots of different types of logics (some CLB, some DSP, some Block-RAM, some IO etc)

[ The device floorplan could be modified in VPR and the placement and routing engine would automatically adapt to generate legal solutions and optimize within those constraints; this would not require any code changes beyond the architecture file description.  The partitioner would need a modified balance criterion where it targeted not resource-balanced partitions, but instead forbade partitions that deviated too much from the resources available in each portion (die) of the system. This would require source code changes to metis.]

2. Once initial placement is done, can you think of performing some physically knowledgeable optimization techniques? For example, if there are lot of connections between different dies, can you perform some minimization operation on that?

[ We rely on VPR's annealer to perform this minimization.  Many random moves are generated, including those of blocks connected by nets that span the interposer. Our modified cost functions make it visible to VPR that moves which reduce cut size, or produce a placement that has a higher potential to later be perturbed to reduce cut size, are preferable. ]  

3. In section V B (partitioning Tool), you assumed 0.05 (5%) in line 30. What is the rationale behind choosing that number? What happens if we choose some other number like 10% or 15%?

[ We have added a discussion of why we choose a relatively tight balance constraint of 5% to Section V.B.  
We assumed a 5% balance constraint to give the partitioner some flexibility to minimize cut size, while still forcing the partitions to be within 5% of balanced so that we can implement designs in fairly full FPGAs (e.g. 90% logic utilization).  If we allowed a more relaxed balance constraint of 10%, then a circuit could be split into 60% and 40% portions, and a 2-die interposer system would need to provide enough circuitry in one partition to support 60% of the design logic. This implies the interposer system would need 120% (2 x 60%) of the logic (or DSP, or RAM if they are the limiting resource) required by the circuit.  This would limit logic utilization to 83% of the system (1/1.2), but many FPGA designs go beyond that level of utilization.] 

[ Javeed: try a balance constraint that reflects the real limits in each die. ] [ DONE? Need to respond.]

4. In line 50 of Section V B, are you giving any special consideration to DSP or Block-RAM etc? Or are those treated the same way you treat simple LUTs and Flops?

[ DSP and block RAM primitives are treated the same way as LUTs and FFs by the partitioner but it does enforce a balance criterion on each resource. ]

5. In your work, maximum how many partitions are allowed? Was there any situation where partitioned could not partition the design?

[ We experiment with 2-die and 4-die systems, which require 2 and 4 partitions respectively. Partitioning was successful on all designs.]

6. In page 13, section E (Impact of Aspect Ratio), can you explain some other different styles?
Like

  die-1      die-2
  die-3      die-4
  die-5      die-6
  die-7      die-8
  
[ Two dimensional arrays of dice are interesting, but we have not studied such systems and they will require some further changes to VPR's architecture generation phase and model of the system. Hence we consider this interesting future work, but beyond the scope of this paper.]

7. What is the frequency loss between monolithic design and multi-die design?

[ This is discussed in Section VI.C. The frequency loss is highly dependent on the interposer delay, but mostly independent of the % wires cut.  For a 2-die system, a 0.5 ns interposer delay reduces frequency by 4% vs. a monolithic FPGA, while a 1 ns interposer delay reduces frequency by 11%.]

8. Any data regarding the runtime comparison of this tool between for a monolithic design and a similar multi-die design? In the FPGA-industry, runtime is a very important issue

[ Ehsan: to run 3 experiments.  conventional VPR, modified VPR on 2-die system with 0% wires cut and 0 ns delay, and modified VPR on 2-die system with 80% wires cut and 1 ns delay.]
  Javeed:  Send CPU time for 2 partition and 4 partition case.

9. Any analyze on the power consumption for multi-die designs? Any comparison of power consumption with respect to that of a monolithic design?

[ This is interesting future work, but we have not investigated the power impact of an interposer. Our intuition is that the power increase will be small as relatively few signals cross the interposer compared to the within-die signals throughout the system, but we have not quantiied this effect. ]


In spite of all these feedbacks, I like your paper. Good job

[ Thank you for your comments!]


Reviewer: 3

In section IV.A, you should mention that VPR's placer is simulated annealing-based. In particular, the reason you cited in the last paragraph of IV.A is based on the assumption that you are using an iterative improvement-based placer (and not, say, an analytical placer).

[Ehsan: add mention of annealing][DONE]

For the discussion about hypergraph to graph transformation in Sec V.C, there were many papers discussing how to set the edge weights in the 90's, please cite them (for example, see the survey paper "Recent directions in netlist partioning: a survey"). In particular, setting the edge weight to 1/(n-1) instead of 1/n is a popular choice since it can model a 2-pin net exactly. So, it makes sense to report the results for 1/(n-1). It looks like your star model picks one node of a net and connect it to other nodes of the net, but the more popular star model in the physical design literature would add a dummy node for each net that connects to all nodes of the net. So please explicitly define your star model in words. And from p.9, it looks like that you also want to distinguish the source of a net from the sinks of a net in the star model, if so, please state that in the definition of your star model.

[Javeed: make explicit and correct text if it says we make all hyperedges the same weight with 1/n (they will vary from 1/2 to 1 in total hyperedge weight).] [DONE]

p.9, middle of left column said "the star topology clearly differentiates sources from sinks and this appears to give the partitioner an anchor point that pulls all hyperedge fanouts toward the source", you should mention that Metis is an iterative improvement-based partitioner which is related to your reasoning.

[ Javeed: add.] [DONE]

p.9, left column, 2nd last paragraph, your star 1 edge weight scheme actually assigns a total weight of n-1 (not n) to each net. Similarly, your star 1/n model assigns a total weight of (n-1)/n (not 1) to each net.

[Javeed: fix.] [DONE]

Some writing issues:

- Sec II.A, 1st paragraph, read "As described in Section III". It is a bit odd since the readers have not read section III at that point.

[Ehsan: take out.][DONE]

- The text never refers to Fig.3.
[Ehsan: add reference][DONE]

- There should not be indentation following eqts. (1), (2), (3), (5), (6).
[Ehsan: fix. Shouldn't start a new paragraph after.][DONE]

- Sec. V.E., "It is slightly worse ... even for ..." -> "But it is slightly worse ... for ..."
[Ehsan: fix.][DONE]

Reviewer: 4

(1) The case for METIS over hMETIS is weak

Both METIS and hMETIS are well known tools for graph and hypergraph partitioning, respectively. Since circuits are known to be hypergraphs, the natural inclination would be to use hMETIS; however, the authors argue that METIS is more appropriate because hMETIS does not support heterogeneous balance constraints. Therefore, the authors take great pains to convert the hypergraph to a graph.

This is not satisfying to me at all. If the correct solution is to use hypergraph partitioning with heterogeneous balance constraints, then the authors should make appropriate modifications to hMETIS to produce the appropriate partitioning tool. It is unclear how much relevant information is lost by transforming the hypergraph to a graph, and what the impact on relative CAD metrics (W_min, f_max, etc) would be.

[Javeed: mention hMetis is not open source (emailed person), give results on hmetis vs. metis cut size (with and without balance), and mention the star model should help timing, as timing is a connection / edge, phenomenon, not a net / hyperedge phenomenon.] [DONE]
The hMetis source code is not freely available. We contacted the authors of hMetis, who stated that an hMetis update is in development and will contain support for heterogeneous balance constraints. Our experiments showed that hMetis performed 30% better on average when compared to Metis on our benchmark circuits, when both were used with homogeneous balance constraints.


(2) I disagree with the case for balance constraints

I do not understand why a good partioning solution is approximately equal among the different dice in an interposer-based FPGA, because doing so intuitively seems as though it would make maximal use of interposer wires, which are both slow and constrained (in terms of general availability).

As a generic example, suppose that I have a two dice, each with 100 CLBs, and the circuit that I am trying to partition has 101 CLBs. Do I really want a 50/51 partition using a large number of interposer wires? I don’t think that I do. If my partition was 100/1 instead, I would probably use just a handful of interposer wires, and the likelihood that the interposer wire is critical (i.e., it is on the f_max path) is fairly low.

Now, I realize that my example is toy, and I also know that you don’t want 100% LUT/CLB utilization (referring to the title of DeHon’s FPGA 1999 paper). So, 100/1 is probably going to be unroutable on the densely used side, by 90/11 or 80/21 may work out well, and would use far fewer interposer wires than 50/51.

In summary, I question the motivation to impose balance constraints. It seems like an unbalanced partition (within the parameters that provide reasonable routability) is the right way to go. Taking points (1) and (2) together (and assuming that I am correct—which I may not be), if balance constraints are unneeded, then the case for METIS over hMETIS falls apart, so perhaps hMETIS (with an appropriate objective) is perhaps a better way to go.

[ We have added a discussion of why we use a balance constraint to Section V.B.  The basic reason is that interposer-based FPGAs are intended to expand FPGA capacity beyond what is possible, or what yields at a reasonable/economic rate, on a single die.  To make use of this increased capacity, however, one must place a significant fraction of the circuit in each die. Stated another way, we wish to examine the routability of circuits at typical FPGA customer utilization levels, which are usually in the 90% range of the limiting resource (usually logic, RAM, or multipliers).  If one uses 90% of the resources in a two-die interposer system, for example, then a 0.05 unbalance constraint allows a partitioning with 45% of the design in one die and 55% in the other.  0.55 * 0.9 = 0.5, meaning that the more full partition / die uses 50% of the two-dice systems resources, which is 100% of the resources available in that die.  A larger unbalance between partitions would mean that a design that used 90% of the interposer system's resources would have a partition that is infeasible on one of the two dice.

You are correct that if we targeted lower utilization rates for the interposer systems we could relax the balance constraint, and this might help routability. However, we do not feel this is the best point at which to evaluate interposer routability. Designers are accustomed to using most of an FPGA's logic, RAM, etc. capacity and will have the same expectation for interposer-based FPGAs.  Commercial monolithic FPGAs are architected to have a high probability of successful routing at utilizations of 90% and even higher. Partially this is because the routing is cheaper and the function blocks more expensive than was assumed in DeHon's 1999 paper and partly this is because an FPGA is usually selected and integrated into a board before the FPGA design is complete. While designers can estimate logic, RAM and multiplier utlization with reasonable accuracy even before a design is complete, routability estimation is very hard, so it would be difficult to choose a chip before a design was 100% complete.]


 Javeed: do the tuned balance / limit ]. [DONE?]

==

At the very end of Section V.F, the authors briefly discuss packing bloat, which “was due almost entirely to poor packing of memory blocks: an interesting area for future work would be to augment the partitioner to understand that some RAM primitives are best kept together in one partition.”

Since I’ve already argued that the paper is somewhat lightweight in terms of technical contribution, perhaps doing a proper investigation of this issue and adding the necessary algorithmic support could bring greater novelty to this paper, rather than leaving it open to future work.

[Thank you for your comment. We agree that this is important future work but feel it is beyond the scope of the current paper as we are already at the limit of 14 pages, and the best way to incorporate such "packing aware partitioning hints" is not clear.  We have explored one method of making the partitioner packing-aware, which is to run partitioning after packing; however, this is an inferior CAD flow as the packer benefits more from the partitioning constraints than the partitioner benefits from the packing constraints. We have added text to Section V.F to higlight this. ].

==

In Section VI.E, there is a short study on the impact of FPGA die aspect ratio on the minimum channel width (W_min). I’d like for the authors to clarify this issue a bit, because of my understanding of the way VPR’s routers work.

Firstly, the modifications to the router cost functions described earlier in the paper were for VPR’s timing driven router. To the best of my knowledge, the timing-driven router is very good at reducing critical path delay, but not particular good at minimizing W_min.

VPR also has a routability-driven router which is very good at minimizing W_min, but not particularly effective at reducing critical path delay.

So, to produce Table I, in principle, the routability-driven router should have been chosen; however, if so, then the cost function used by the routability-driven router needs to be modified as well in order to model the interposer and interface at the cut lines.

This leads to a few interdependent questions:
(1) Which router was used?
(2) If the timing-driven router was used, is it justifiable to report W_min values? Should the routability-driven router (enhanced as needed) have been used instead?
(3) If the routability-driven router was used, how were the cost functions enhanced?

[We used the VPR timing-driven router for all results in this paper, and believe it is the correct choice as it produces a better combination of routability and timing than VPR's routability-driven router, and its algorithm is more scalable to large circuits.

  The timing-driven router in VPR optimizes for both timing and routability (timing is optimized as much as possible, but the cost of congestion grows to eventually swamp the timing terms in congested parts of the design if necessary to achieve routability).  Hence its routability is good; in "Architecture and CAD for Deep Submicron FPGAs" by Betz, Marquardt and Rose, for example, the timing driven router was found to produce circuits with a 6% higher Wmin, but that had an operating frequency 2.58x higher.  Hence it achieves a superior combination of routability and timing than the routability-driven router, although you are correct that the routability-driven router produces slightly lower absolute channel widths.  A further problem with the routability-driven router in VPR is that its algorithm does not scale as well as the timing-driven router, and hence its runtime will be much higher and likely impractical on the larger circuits we use.   On the largest circuit for which results are presented in "Architecture and CAD for Deep-Submicron FPGAs" the VPR routability-driven router requires 14.6x more CPU time than the timing-driven router, and the CPU time gap increases with circuit size.  Hence on the much larger circuits (>10x larger) we use the CPU time of the routability-driven router would make it very difficult to use.

For these reasons, the timing-driven router in VPR is the default and is normally used to find both the minimum channel width and the achievable timing of circuits; in the recent VTR 7.0 paper for example, only results for the timing-driven VPR router are given as the routability-driven router is not widely used.]

==

Minor points:

Section I, third paragraph
- Don’t use [3] as a word. Either say “Ref. [3]” or “Alexander et al. [3]”
- et al should be “et al.” (there is a period after al, but not after et)

[Thanks you; we have fixed this text.]

Regarding the placer timing cost described in Section IV.A.1), does the delay term include the multiplexers or just the bumps and the wires through the interposer?
[Ehsan: modify text.][DONE]

Regarding Equation (3), can you give a better explanation of t  he W_chanx and W_chany terms? I didn’t completely understand them from the short 1-sentence description.
[Ehsan: change.][DONE]

Below Equation (6), I don’t understand the ratio_of_wires_cut metric, especially vis-a-vis Figure 6.
[Use % wires cut instead][DONE]

In Figure 6, the green box and label (a) are nearly impossible to see given the light blue background with white spacing. Please switch to a darker shade of green.
[Ehsan: fix][DONE]

In Equation (1), does the ExpectInterposeHops include the delay of the muxes?
[Ehsan: explain / fix][DONE: ExpectInterposeHops is not the delay term. It's the number of expected interposer hops which is multiplied by the total delay of each hop. I changed the variable name to: NumExpectedInterpHops]


Fig. 23 is somehow redundant, as it is simply a different way to display data that was reported earlier.
[You are correct that Fig. 23 is simply a different way to visualize the data of Fig. 22. We prefer to retain the figure however, as it highlights that the interposer wiring capacity becomes the dominant factor in routability when its absolute signal count drops below a certain level. Other figures show the within-die channel width required when one cuts a certain percentage of the normal FPGA wiring across the interposer, while this figure presents the same data as within-die channel width required vs. the absolute (not relative) interposer wiring capacity. We believe this highlights the important fact that the interposer for this FPGA architecture requires about 20 wires per vertical channel at the absolute minimum.]
